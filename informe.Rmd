---
title: "Teoría Estadística - Distribución Binomial"
author: "Gonzalo Barrera Borla y Jesús Zapata"
date: "2 de Septiembre de 2018"
output:
  html_document: 
    df_print: kable
    number_sections: yes
    theme: journal
    toc: yes
  pdf_document: 
    latex_engine: xelatex
header-includes:
- \usepackage{mathcal}
---

# Introducción

Sea $\mathbb{X}=(X_1,X_2,...,X_n)$ un vector aleatorio tal que $ X_i \sim Bi(k, p) \ \forall \ i \in \{1,...,n\}$, y $X_i \ \bot \ X_j \ \forall \ i,j \in \{1,...,n\}$. Es decir, que todas las $X_i$ son independientes entre sí, están idénticamente distribuidas, y su función de distribución pertenece a la "familia binomial"

Si escribimos $\theta = (k, p)$ la función de probabilidad puntual queda dada por

$$
f(x; \theta) = \binom{k}{x} \ p^{x} \ (1-p)^{k-x}
$$
La función de densidad conjunta, llamémosla $\textbf{p}(x_1,x_2,...,x_n; \theta)$ será entonces

$$
\textbf{p}(x_1,x_2,...,x_n; \theta) = \prod_{i=1}^{n}{f(x_i; \theta)} = \left[\prod_{i=1}^{n}{\binom{k}{x_i}}\right] \times p^{\sum_{i=1}^{n}{x_i}} \times (1-p)^{nk - \sum_{i=1}^{n}{x_i}}
$$
Si convenimos en la notación $\overline{x} = \frac{1}{n}\sum_{i=1}^{n}{x_i} $, podemos reescribir

$$
\textbf{p}(x_1,x_2,...,x_n; \theta) = \left[\prod_{i=1}^{n}{\binom{k}{x_i}}\right] \times p^{n\overline{x}} \times (1-p)^{n(k - \overline{x})}
$$

# Estimadores

Consideraremos tres tipos de situaciones:
- estimación del número de pruebas $k$ con probabilidad de éxito $p$ conocida,
- estimación de $p$ con $k$ conocido, y
- estimación de $\theta = (k, p)$, sin ningún parámetro conocido.

## De $q(\theta) = k$

Asumiendo que la probabilidad de éxito $p$ de un ensayo individual es conocida, 
es razonable querer estimar el parámetro desconocido $k$, el número de ensayos
efectuados, siempre el mismo para cada variable aleatoria $X_i$

Lo intentaremos según el método de los momentos, usando los momentos de primer y segundo orden, y según el método de máxima verosimilitud.

### Momentos con $g(x) = x$

Sea $g$ una función de $\mathbb{R}$ en $\mathbb{R}$, luego el método de los momentos estima $\theta$, por el valor $\hat{\theta} = \delta(\mathbb{X})$ que satisface la ecuación

$$
\frac{1}{n}{\sum_{i=1}^{n}{g(X_i)}}=E(g(X_1) \ | \ \theta = \hat{\theta})
$$

Si usamos $g(x)=x$, y escribimos $\theta = (\hat{k}, p)$ y sabiendo que $E(X_i)=k\times p \ \forall \ i \in \{1,...,n\}$, 
 podemos afirmar que el estimador de $k$ será el $\hat{k}$ que satisfaga
 
$$
\begin{aligned}
\frac{1}{n}{\sum_{i=1}^{n}{X_i}}&=E(X_1 \ | \ \theta = (\hat{k},p)) \\
\overline{X} &= \widehat{k} \times p \\
\widehat{k_{_m_{1}}} &= \frac{\overline{X}}{p}
\end{aligned}
$$
 
### Momentos con $g(x) = x^2$

### MV

## De g(tita) = p

### M1

### M2

### MV

## De g(tita) = (k, p)

### M12

### MV

## Comparación

### Error Cuadrático Medio

### Estadísticos suficientes
- ecm = sesgo y varianza
- consistencia
- estadístico suficiente

###Estimador de momentos para p con k conocido
Sea $X_{1},X_{2},X_{3},...,X_{n}$ una muestra aleatoria con distribuci?n $Bi(p,k)$. Consideremos $g(x)=x$

$$
\frac{1}{n}\sum_{i=1}^{n}X_{i}=k\,p
$$
Por lo tanto, el estimador de momentos de p es:
$$
\hat p=\frac{1}{nk}\sum_{i=1}^{n}X_{i}
$$
$$
\hat p_{mo}=\frac{1}{k}\overline{X}
$$

###Estimador de de m?xima verosimilitud para p con k conocido
Sea $X_{1},X_{2},X_{3},...,X_{n}$ una muestra aleatoria con distribuci?n $Bi(k,p)$.

$$
p(x,p)=\binom{x}{p} p^x (1-p)^{k-x}
$$

La funci?n de probabilidad conjunta es
$$
L(x,p)=\prod_{i=1}^{k}\binom{x}{p} p^x (1-p)^{k-x}
$$
$$
L(x,p)=\prod_{i=1}^{k}\binom{x}{p}*[ p^{\sum_{i=1}^{n}X_{i}}(1-p)^{nk-\sum_{i=1}^{n}X_{i}}]
$$
$$
\frac{\partial ln L(x,p)}{\partial p}=\frac{\sum_{i=1}^{n}X_{i}}{p}-\frac{nk-\sum_{i=1}^{n}X_{i}}{1-p}
$$
Escrito de otra forma es:
$$
\frac{\partial ln L(x,p)}{\partial p}=\frac{k n \overline{X}}{p}-\frac{n(k-\overline{X})}{(1-p)}
$$
Igualando a cero
$$
\frac{n \overline{X}(1-p)-np(k-\overline{X})}{p(1-p)}=0
$$

$$
n [\overline{X}(1-p)-p(k-\overline{X})]=0
$$

$$
\overline{X}(1-p)-p(k-\overline{X})=0
$$

$$
\overline{X}-\overline{X}p-pk+\overline{X}p=0
$$

$$
\overline{X}-pk=0
$$
Luego,el estimador de maxima verosimilitud es:
$$
\hat p_{mv}=\frac{1}{k}\overline{X}
$$

### Error cuadr?tico medio

$$
ECM_{\theta}=E_{\theta}(\delta(X)-q(\theta))^2
$$

Para determinar la bondad del estimador se comparan los ECM de cada estimador y el que tiene menor ECM es el mejor estimador, es decir:

$$
ECM_{\theta}(\delta^*)\leq ECM_{\theta}(\delta) \,\, \, \forall \theta \in \Theta
$$

En nuestro caso no se puede comparar los estimadores porque tanto el de momentos como el de maxima verosimilitud nos di? el mismo estimador. De cualquier manera se puede determinar el  ECM de los mismos.

Para calcular el ECM utilizaremos la siguiente f?rmula 

$$
ECM_{\theta}=Var_{\theta}(\delta(X))+ sesgo
$$

### Sesgo

El sesgo de un estimador es
$$
Sesgo=\mathbb{E}_{\theta}(\delta(X)-q(\theta))
$$

Un estimador es insesgado cuando el sesgo es cero, es decir que el $ECM_{\theta}=Var_{\theta}(\delta(X))$

Por definici?n de sesgo,un es estimador es insesgado, si y s?lo si $E_{\theta}(\delta(X))=q(\theta)$

En nuestro caso:
$$
\mathbb{E}[\hat p]=\mathbb{E}[\frac{\overline{X}}{n}]
$$
$$
\frac{1}{n}.\mathbb{E}(X_{1})=\frac{1}{n}.n.p= p
$$
Por lo tanto el estimador es insesgado

### $Var_{\theta}(\delta(X))$

En nuestro caso:

$$
V[\hat p]=V[\frac{\overline X}{n}]=\frac{V(\overline X)}{n^2}
$$
Primero se determinar? la $V(\overline X)$
$$
V(\overline X)=V(\frac{1}{k}\sum_{i=1}^{n}X_{i}]
$$
$$
V(\overline X)=\frac{1}{k^2}. k.V(X_{i})
$$
$$
V(\hat p)=\frac{1}{k.n}.p (1-p)
$$

Por lo tanto el Error cuadr?tico medio es:$ECM=\frac{1}{k.n}.p (1-p)$

### Estadísticos suficientes

Sea $X$ un vector aleatorio de dimensi?n n cuya distribuci?n es$F(x,\theta) con \theta \in \Theta$
Se dice que un estad?stico $T=r(X)$ es suficiente para todo $\theta$ si la distribuci?n de $X$ condicional a que $T=t$ es independiente de $\theta$ para todo $t$

Para encontrar el estad?stico suficiente se utilizar? el siguiente teorema

###Teorema de Factorizaci?n

Sea $X$ un vector aleatorio con funci?n de densidad o funci?n de probabilidad puntural $p(x,\theta),\theta \in \Theta$. Entonces, el estad?stico $T=r(X)$ es suficiente para $\theta$si y s?lo si existen dos funciones $g$ y $h$ tales que:

$$
p(\mathbf{x},\theta)=g(r(\mathbf{x}),\theta) h(\mathbf{x})
$$



library(tidyverse)
library(pryr)

sims <- 1e2
max_n <- 1e3
lambda <- 1


```{r maxima_verosimilitud}
verosimilitud <- function(X) {
  function(tita) {
      sum(log(dbinom(X, size = tita[["n"]], prob = tita[["p"]])))
        }
	} 

tita <- list(n = 50, p = 0.23)
muestra <- rbinom(100, size = tita[["n"]], prob = tita[["p"]])
ver1 <- verosimilitud(muestra)
espacio <- cross(list(
  n = 1:100,
    p = (1:100)/100
    ))

tibble(titon = espacio,
       vero = map_dbl(titon, ver1)) %>%
         transmute(
	     n = map_dbl(titon, "n"),
	         p = map_dbl(titon, "p"),
		     vero) -> resumen

resumen %>% mutate(E = n*p) %>% arrange(desc(vero))
resumen %>% 
  filter(near(n*p, 0.6)) %>%
    arrange(desc(vero))

resumen %>%
  ggplot(aes(n, p, z = vero)) +
    geom_contour(binwidth = 0.1)

resumen %>%
  ggplot(aes(n, p)) +
    geom_density2d()

espacio_vero
max_n <- 10000
map(espacio, ver1)
ver1(list(n = 50, p = 0.1))
data <- tibble(
  x = seq_len(max_n),
    y = map_dbl(x, ver1)) 

data %>%
  ggplot(aes(x, y)) +
    geom_line()

data %>% arrange(desc(y))
```

```{r computo_ecm}
estimadores <- list(
  m1_p = function(X, n, ...) { mean(X) / n },
    m2_p = function(X, n, ...) {
        (n + sqrt(n^2 + 4*n*(n-1) * mean(X^2))
	     ) / (2 * n * (n - 1))
	       },
	         m1_n = function(X, p, ...) { mean(X) / p },
		   m2_n = function(X, p, ...) {
		       (sqrt(p^2 - 2*p + 4*mean(X^2) + 1) + p - 1
		            ) / (2 * p)
			      },
			        m12_n = function(X, ...) {mean(X)^2 / (mean(X)^2 + mean(X) - mean(X^2))},
				  m12_p = function(X, ...) {(mean(X)^2 + mean(X) - mean(X^2)) / mean(X)},
				    mv_p = function(X, p, ...) { mean(X) / p }
				    )

rango_p_final <- c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 0.5, 0.7, 0.9, 0.97, 0.99, 0.997, 0.999, 1)
rango_p <- c(0, 0.01, 0.1, 0.5, 0.9, 0.99, 1)
rango_n_final <- round(exp(seq_len(9)), 0)
rango_n <- round(exp(seq_len(4)), 0)
rango_k <-rango_n
sims <- 1000

estimaciones_a_realizar <- cross_df(list(
  k = rango_k, estimador = names(estimadores))
  )

df <- cross_df(list(n = rango_n, p = rango_p, sim_id = seq_len(sims))) %>%
  mutate(
     data = map2(n, p, ~rbinom(n = max(rango_k), size = .x, prob = .y)))

asistente_estimacion <- function(X, nombre_estimador, k, ...) {
  estimadores[[nombre_estimador]](X[seq_len(k)], ...)
  }

resultados <- crossing(df, estimaciones_a_realizar) %>%
  mutate(valor = pmap_dbl(
      .l = list(X = data, nombre_estimador = estimador,k = k, n = n, p = p),
          .f = asistente_estimacion))

pryr::object_size(resultados)       

resultados %>%
  filter(n == 20, p == 0.001, estimador %in% c("m1_p", "m2_p", "m12_p")) %>%
    ggplot(aes(valor, color = estimador)) +
      geom_freqpoly()

resultados %>% 
  select(-data) %>% 
    write_csv("resultados.csv")

resultados %>%
  group_by(n, p, k, estimador) %>%
    summarise(media = mean(valor)) %>%
                #varianza = sd(valor)) %>%
		  spread(estimador, media) %>%
		    View()

  ggplot(aes(valor, color = k)) +
    geom_freqpoly() + 
        facet_grid(n ~ p)


```